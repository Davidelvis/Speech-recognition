{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256f54bb-9b88-4d01-aee6-ee4e328a9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c46c00-97d4-477e-8480-aee1e531b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile\n",
    "import json\n",
    "\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import librosa\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "\n",
    "import _pickle as pickle\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "from keras.layers import (Input, Lambda)\n",
    "#from keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "import os\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4762b094-f863-4d36-bf58-56adce7c3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.tracking._DEFAULT_USER_ID = 'https://github.com/Davidelvis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8dc3fc-02cc-4f93-a25e-c64e0fe8f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b647c0c-de7a-4990-b3ae-c1dcb37302ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map_str = \"\"\"\n",
    " ' 0\n",
    " <SPACE> 1\n",
    " a 2\n",
    " b 3\n",
    " c 4\n",
    " d 5\n",
    " e 6\n",
    " f 7\n",
    " g 8\n",
    " h 9\n",
    " i 10\n",
    " j 11\n",
    " k 12\n",
    " l 13\n",
    " m 14\n",
    " n 15\n",
    " o 16\n",
    " p 17\n",
    " q 18\n",
    " r 19\n",
    " s 20\n",
    " t 21\n",
    " u 22\n",
    " v 23\n",
    " w 24\n",
    " x 25\n",
    " y 26\n",
    " z 27\n",
    " N 15\n",
    " U 22\n",
    " K 12\n",
    " < 0\n",
    " > 0\n",
    " _ 0\n",
    " - 0\n",
    " . 0\n",
    " 1 0\n",
    " 2 0\n",
    " 3 0\n",
    " 4 0\n",
    " 5 0\n",
    " 6 0\n",
    " 7 0\n",
    " 8 0\n",
    " 9 0\n",
    " ? 0\n",
    " 0 0\n",
    " \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37721e18-5d50-49a9-ab96-eeabad28edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map = {}\n",
    "index_map = {}\n",
    "for line in char_map_str.strip().split('\\n'):\n",
    "  ch, index = line.split()\n",
    "  char_map[ch] = int(index)\n",
    "  index_map[int(index)] = ch\n",
    "  index_map[1] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "350836bd-1e79-4794-8771-b09a46b450cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feat_dim(window, max_freq):\n",
    "    return int(0.001 * window * max_freq) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd5208c-374b-4298-b542-bc3c3079546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
    "    \"\"\"\n",
    "    Compute the spectrogram for a real signal.\n",
    "    The parameters follow the naming convention of\n",
    "    matplotlib.mlab.specgram\n",
    "\n",
    "    Args:\n",
    "        samples (1D array): input audio signal\n",
    "        fft_length (int): number of elements in fft window\n",
    "        sample_rate (scalar): sample rate\n",
    "        hop_length (int): hop length (relative offset between neighboring\n",
    "            fft windows).\n",
    "\n",
    "    Returns:\n",
    "        x (2D array): spectrogram [frequency x time]\n",
    "        freq (1D array): frequency of each row in x\n",
    "\n",
    "    Note:\n",
    "        This is a truncating computation e.g. if fft_length=10,\n",
    "        hop_length=5 and the signal has 23 elements, then the\n",
    "        last 3 elements will be truncated.\n",
    "    \"\"\"\n",
    "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
    "\n",
    "    window = np.hanning(fft_length)[:, None]\n",
    "    window_norm = np.sum(window**2)\n",
    "\n",
    "    # The scaling below follows the convention of\n",
    "    # matplotlib.mlab.specgram which is the same as\n",
    "    # matlabs specgram.\n",
    "    scale = window_norm * sample_rate\n",
    "\n",
    "    trunc = (len(samples) - fft_length) % hop_length\n",
    "    x = samples[:len(samples) - trunc]\n",
    "\n",
    "    # \"stride trick\" reshape to include overlap\n",
    "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
    "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
    "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
    "\n",
    "    # window stride sanity check\n",
    "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
    "\n",
    "    # broadcast window, compute fft over columns and square mod\n",
    "    x = np.fft.rfft(x * window, axis=0)\n",
    "    x = np.absolute(x)**2\n",
    "\n",
    "    # scale, 2.0 for everything except dc and fft_length/2\n",
    "    x[1:-1, :] *= (2.0 / scale)\n",
    "    x[(0, -1), :] /= scale\n",
    "\n",
    "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
    "\n",
    "    return x, freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65de5224-b558-4117-bcd2-7e608836da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
    "                          eps=1e-14):\n",
    "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
    "    Params:\n",
    "        filename (str): Path to the audio file\n",
    "        step (int): Step size in milliseconds between windows\n",
    "        window (int): FFT window size in milliseconds\n",
    "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "            [0, max_freq] are returned\n",
    "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
    "    \"\"\"\n",
    "    with soundfile.SoundFile(filename) as sound_file:\n",
    "        audio = sound_file.read(dtype='float32')\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if audio.ndim >= 2:\n",
    "            audio = np.mean(audio, 1)\n",
    "        if max_freq is None:\n",
    "            max_freq = sample_rate / 2\n",
    "        if max_freq > sample_rate / 2:\n",
    "            raise ValueError(\"max_freq must not be greater than half of \"\n",
    "                             \" sample rate\")\n",
    "        if step > window:\n",
    "            raise ValueError(\"step size must not be greater than window size\")\n",
    "        hop_length = int(0.001 * step * sample_rate)\n",
    "        fft_length = int(0.001 * window * sample_rate)\n",
    "        pxx, freqs = spectrogram(\n",
    "            audio, fft_length=fft_length, sample_rate=sample_rate,\n",
    "            hop_length=hop_length)\n",
    "        ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a7acab3-292c-4fec-931b-efc53c758897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_int_sequence(text):\n",
    "    \"\"\" Convert text to an integer sequence \"\"\"\n",
    "    int_sequence = []\n",
    "    for c in text:\n",
    "        if c == ' ':\n",
    "            ch = char_map['<SPACE>']\n",
    "        else:\n",
    "            # print(\"checking character \" + c + \" in map:\")\n",
    "            # print(char_map)\n",
    "            ch = char_map[c]\n",
    "        int_sequence.append(ch)\n",
    "    return int_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3132dec2-538e-49e9-9648-0fd2432ba44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_sequence_to_text(int_sequence):\n",
    "    \"\"\" Convert an integer sequence to text \"\"\"\n",
    "    text = []\n",
    "    for c in int_sequence:\n",
    "        ch = index_map[c]\n",
    "        text.append(ch)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f1a663f-c41d-4fcb-b9c8-de682047a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://martin-thoma.com/word-error-rate-calculation/\n",
    "def wer(r, h):\n",
    "    \"\"\"\n",
    "    Calculation of WER with Levenshtein distance.\n",
    "\n",
    "    Works only for iterables up to 254 elements (uint8).\n",
    "    O(nm) time ans space complexity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : list\n",
    "    h : list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
    "    1\n",
    "    >>> wer(\"who is there\".split(), \"\".split())\n",
    "    3\n",
    "    >>> wer(\"\".split(), \"who is there\".split())\n",
    "    3\n",
    "    \"\"\"\n",
    "    # initialisation\n",
    "    import numpy\n",
    "    d = numpy.zeros((len(r)+1)*(len(h)+1), dtype=numpy.uint8)\n",
    "    d = d.reshape((len(r)+1, len(h)+1))\n",
    "    for i in range(len(r)+1):\n",
    "        for j in range(len(h)+1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitution = d[i-1][j-1] + 1\n",
    "                insertion    = d[i][j-1] + 1\n",
    "                deletion     = d[i-1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    return d[len(r)][len(h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d97de87-04d4-43e0-b553-3e932516d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for plotting\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hist(p):\n",
    "    hist = pickle.load(open( \"models/\" + p + \".pickle\", \"rb\"))\n",
    "    plt.plot(hist['loss'], label=\"train\")\n",
    "    plt.plot(hist['val_loss'], label=\"valid\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac554193-86f4-4730-b1bc-3a5f852f3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG_SEED = 123\n",
    "\n",
    "def make_audio_gen(train_json,\n",
    "                   valid_json,\n",
    "                   minibatch_size=20,\n",
    "                   spectrogram=True,\n",
    "                   mfcc_dim=13,\n",
    "                   sort_by_duration=False,\n",
    "                   max_duration=10.0):\n",
    "    return AudioGenerator(train_json, valid_json, minibatch_size=minibatch_size, \n",
    "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
    "        sort_by_duration=sort_by_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56215ccf-dfc3-40fd-968e-3b65deb16d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines a class that is used to featurize audio clips, and provide\n",
    "them to the network for training or testing.\n",
    "\"\"\"\n",
    "class AudioGenerator():\n",
    "    def __init__(self, train_corpus, valid_corpus, step=10, window=20, max_freq=8000, mfcc_dim=13,\n",
    "        minibatch_size=20, desc_file=None, spectrogram=True, max_duration=10.0, \n",
    "        sort_by_duration=False):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            step (int): Step size in milliseconds between windows (for spectrogram ONLY)\n",
    "            window (int): FFT window size in milliseconds (for spectrogram ONLY)\n",
    "            max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "                [0, max_freq] are returned (for spectrogram ONLY)\n",
    "            desc_file (str, optional): Path to a JSON-line file that contains\n",
    "                labels and paths to the audio files. If this is None, then\n",
    "                load metadata right away\n",
    "        \"\"\"\n",
    "        self.train_corpus = train_corpus\n",
    "        self.valid_corpus = valid_corpus\n",
    "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
    "        self.mfcc_dim = mfcc_dim\n",
    "        self.feats_mean = np.zeros((self.feat_dim,))\n",
    "        self.feats_std = np.ones((self.feat_dim,))\n",
    "        self.rng = random.Random(RNG_SEED)\n",
    "        if desc_file is not None:\n",
    "            self.load_metadata_from_desc_file(desc_file)\n",
    "        self.step = step\n",
    "        self.window = window\n",
    "        self.max_freq = max_freq\n",
    "        self.cur_train_index = 0\n",
    "        self.cur_valid_index = 0\n",
    "        self.cur_test_index = 0\n",
    "        self.max_duration=max_duration\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.spectrogram = spectrogram\n",
    "        self.sort_by_duration = sort_by_duration\n",
    "\n",
    "    def get_batch(self, partition):\n",
    "        \"\"\" Obtain a batch of train, validation, or test data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            audio_paths = self.train_audio_paths\n",
    "            cur_index = self.cur_train_index\n",
    "            texts = self.train_texts\n",
    "        elif partition == 'valid':\n",
    "            audio_paths = self.valid_audio_paths\n",
    "            cur_index = self.cur_valid_index\n",
    "            texts = self.valid_texts\n",
    "        elif partition == 'test':\n",
    "            audio_paths = self.test_audio_paths\n",
    "            cur_index = self.test_valid_index\n",
    "            texts = self.test_texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "        features = [self.normalize(self.featurize(a)) for a in \n",
    "            audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
    "\n",
    "        # calculate necessary sizes\n",
    "        max_length = max([features[i].shape[0] \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        max_string_length = max([len(texts[cur_index+i]) \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        \n",
    "        # initialize the arrays\n",
    "        X_data = np.zeros([self.minibatch_size, max_length, \n",
    "            self.feat_dim*self.spectrogram + self.mfcc_dim*(not self.spectrogram)])\n",
    "        labels = np.ones([self.minibatch_size, max_string_length]) * 28\n",
    "        input_length = np.zeros([self.minibatch_size, 1])\n",
    "        label_length = np.zeros([self.minibatch_size, 1])\n",
    "        \n",
    "        for i in range(0, self.minibatch_size):\n",
    "            # calculate X_data & input_length\n",
    "            feat = features[i]\n",
    "            input_length[i] = feat.shape[0]\n",
    "            X_data[i, :feat.shape[0], :] = feat\n",
    "\n",
    "            # calculate labels & label_length\n",
    "            label = np.array(text_to_int_sequence(texts[cur_index+i])) \n",
    "            labels[i, :len(label)] = label\n",
    "            label_length[i] = len(label)\n",
    " \n",
    "        # return the arrays\n",
    "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
    "        inputs = {'the_input': X_data, \n",
    "                  'the_labels': labels, \n",
    "                  'input_length': input_length, \n",
    "                  'label_length': label_length \n",
    "                 }\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    def shuffle_data_by_partition(self, partition):\n",
    "        \"\"\" Shuffle the training or validation data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = shuffle_data(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "            self.train_length = len(self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = shuffle_data(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "            self.valid_length = len(self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def sort_data_by_duration(self, partition):\n",
    "        \"\"\" Sort the training or validation sets by (increasing) duration\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = sort_data(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = sort_data(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def next_train(self):\n",
    "        \"\"\" Obtain a batch of training data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('train')\n",
    "            self.cur_train_index += self.minibatch_size\n",
    "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
    "                self.cur_train_index = 0\n",
    "                self.shuffle_data_by_partition('train')\n",
    "            yield ret    \n",
    "\n",
    "    def next_valid(self):\n",
    "        \"\"\" Obtain a batch of validation data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('valid')\n",
    "            self.cur_valid_index += self.minibatch_size\n",
    "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
    "                self.cur_valid_index = 0\n",
    "                self.shuffle_data_by_partition('valid')\n",
    "            yield ret\n",
    "\n",
    "    def next_test(self):\n",
    "        \"\"\" Obtain a batch of test data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('test')\n",
    "            self.cur_test_index += self.minibatch_size\n",
    "            if self.cur_test_index >= len(self.test_texts) - self.minibatch_size:\n",
    "                self.cur_test_index = 0\n",
    "            yield ret\n",
    "\n",
    "    def load_train_data(self):\n",
    "        desc_file=self.train_corpus\n",
    "        self.load_metadata_from_desc_file(desc_file, 'train')\n",
    "        self.fit_train()\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('train')\n",
    "\n",
    "    def load_validation_data(self):\n",
    "        desc_file=self.valid_corpus\n",
    "        self.load_metadata_from_desc_file(desc_file, 'validation')\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('valid')\n",
    "\n",
    "    def load_test_data(self):\n",
    "        desc_file='test_corpus.json'\n",
    "        self.load_metadata_from_desc_file(desc_file, 'test')\n",
    "    \n",
    "    def load_metadata_from_desc_file(self, desc_file, partition):\n",
    "        \"\"\" Read metadata from a JSON-line file\n",
    "            (possibly takes long, depending on the filesize)\n",
    "        Params:\n",
    "            desc_file (str):  Path to a JSON-line file that contains labels and\n",
    "                paths to the audio files\n",
    "            partition (str): One of 'train', 'validation' or 'test'\n",
    "        \"\"\"\n",
    "        audio_paths, durations, texts = [], [], []\n",
    "        with open(desc_file, encoding='utf-8') as json_line_file:\n",
    "            for line_num, json_line in enumerate(json_line_file):\n",
    "                try:\n",
    "                    spec = json.loads(json_line)\n",
    "                    if float(spec['duration']) > self.max_duration:\n",
    "                        continue\n",
    "                    audio_paths.append(spec['key'])\n",
    "                    durations.append(float(spec['duration']))\n",
    "                    texts.append(spec['text'])\n",
    "                except Exception as e:\n",
    "                    # Change to (KeyError, ValueError) or\n",
    "                    # (KeyError,json.decoder.JSONDecodeError), depending on\n",
    "                    # json module version\n",
    "                    print('Error reading line #{}: {}'\n",
    "                                .format(line_num, json_line))\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths = audio_paths\n",
    "            self.train_durations = durations\n",
    "            self.train_texts = texts\n",
    "        elif partition == 'validation':\n",
    "            self.valid_audio_paths = audio_paths\n",
    "            self.valid_durations = durations\n",
    "            self.valid_texts = texts\n",
    "        elif partition == 'test':\n",
    "            self.test_audio_paths = audio_paths\n",
    "            self.test_durations = durations\n",
    "            self.test_texts = texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition to load metadata. \"\n",
    "             \"Must be train/validation/test\")\n",
    "            \n",
    "    def fit_train(self, k_samples=100):\n",
    "        \"\"\" Estimate the mean and std of the features from the training set\n",
    "        Params:\n",
    "            k_samples (int): Use this number of samples for estimation\n",
    "        \"\"\"\n",
    "        k_samples = min(k_samples, len(self.train_audio_paths))\n",
    "        samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
    "        feats = [self.featurize(s) for s in samples]\n",
    "        feats = np.vstack(feats)\n",
    "        self.feats_mean = np.mean(feats, axis=0)\n",
    "        self.feats_std = np.std(feats, axis=0)\n",
    "        \n",
    "    def featurize(self, audio_clip):\n",
    "        \"\"\" For a given audio clip, calculate the corresponding feature\n",
    "        Params:\n",
    "            audio_clip (str): Path to the audio clip\n",
    "        \"\"\"\n",
    "        if self.spectrogram:\n",
    "            return spectrogram_from_file(\n",
    "                audio_clip, step=self.step, window=self.window,\n",
    "                max_freq=self.max_freq)\n",
    "        else:\n",
    "            (rate, sig) = wav.read(audio_clip)\n",
    "            return mfcc(sig, rate, numcep=self.mfcc_dim)\n",
    "\n",
    "    def normalize(self, feature, eps=1e-14):\n",
    "        \"\"\" Center a feature using the mean and std\n",
    "        Params:\n",
    "            feature (numpy.ndarray): Feature to normalize\n",
    "        \"\"\"\n",
    "        return (feature - self.feats_mean) / (self.feats_std + eps)\n",
    "\n",
    "    def train_length(self):\n",
    "        return len(self.train_texts)\n",
    "    \n",
    "    def valid_length(self):\n",
    "        return len(self.valid_texts)\n",
    "\n",
    "\n",
    "def shuffle_data(audio_paths, durations, texts):\n",
    "    \"\"\" Shuffle the data (called after making a complete pass through \n",
    "        training or validation data during the training process)\n",
    "    Params:\n",
    "        audio_paths (list): Paths to audio clips\n",
    "        durations (list): Durations of utterances for each audio clip\n",
    "        texts (list): Sentences uttered in each audio clip\n",
    "    \"\"\"\n",
    "    p = np.random.permutation(len(audio_paths))\n",
    "    audio_paths = [audio_paths[i] for i in p] \n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "def sort_data(audio_paths, durations, texts):\n",
    "    \"\"\" Sort the data by duration \n",
    "    Params:\n",
    "        audio_paths (list): Paths to audio clips\n",
    "        durations (list): Durations of utterances for each audio clip\n",
    "        texts (list): Sentences uttered in each audio clip\n",
    "    \"\"\"\n",
    "    p = np.argsort(durations).tolist()\n",
    "    audio_paths = [audio_paths[i] for i in p]\n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "def vis_train_features(index=0):\n",
    "    \"\"\" Visualizing the data point in the training set at the supplied index\n",
    "    \"\"\"\n",
    "    # obtain spectrogram\n",
    "    audio_gen = AudioGenerator(spectrogram=True)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_audio_path = audio_gen.train_audio_paths[index]\n",
    "    vis_spectrogram_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # obtain mfcc\n",
    "    audio_gen = AudioGenerator(spectrogram=False)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_mfcc_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # obtain text label\n",
    "    vis_text = audio_gen.train_texts[index]\n",
    "    # obtain raw audio\n",
    "    vis_raw_audio, _ = librosa.load(amharic_path(vis_audio_path))\n",
    "    # print total number of training examples\n",
    "    print('There are %d total training examples.' % len(audio_gen.train_audio_paths))\n",
    "    # return labels for plotting\n",
    "    return vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path\n",
    "\n",
    "\n",
    "def plot_raw_audio(vis_raw_audio, title='Audio Signal', size=(12, 3)):\n",
    "    fig = plt.figure(figsize=size)\n",
    "    ax = fig.add_subplot(111)\n",
    "    steps = len(vis_raw_audio)\n",
    "    ax.plot(np.linspace(1, steps, steps), vis_raw_audio)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "def plot_mfcc_feature(vis_mfcc_feature):\n",
    "    # plot the MFCC feature\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized MFCC')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('MFCC Coefficient')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectrogram_feature(vis_spectrogram_feature):\n",
    "    # plot the normalized spectrogram\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_spectrogram_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized Spectrogram')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('Frequency')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49ce25b6-48e6-45f7-bd32-c623f5ee0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(input_dim, units, activation, output_dim=29):\n",
    "    \"\"\" Build a recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Add recurrent layer\n",
    "    simp_rnn = GRU(units, activation=activation,\n",
    "        return_sequences=True, implementation=2, name='rnn')(input_data)\n",
    "    # TODO: Add batch normalization \n",
    "    bn_rnn = BatchNormalization()(simp_rnn)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='/content/models/model_1.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "985b3df1-16fc-45b1-9028-3d44cd361a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8f624e1-664d-4d45-8be0-0bd92f0f6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ctc_loss(input_to_softmax):\n",
    "  # with mlflow.start_run():\n",
    "  the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
    "  input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
    "  label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
    "  output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
    "  # CTC loss is implemented in a lambda layer\n",
    "  loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
    "      [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
    "  model = Model(\n",
    "      inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
    "      outputs=loss_out)\n",
    "    # mlflow.log_param(\"Loss\", loss_out)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "975b8aa3-2444-4ec3-8dfe-77ff0b9e9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(audio_gen,\n",
    "          input_to_softmax, \n",
    "          model_name,\n",
    "          minibatch_size=20,\n",
    "          optimizer=SGD(learning_rate=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
    "          epochs=20,\n",
    "          verbose=1):    \n",
    "    # calculate steps_per_epoch\n",
    "    num_train_examples=len(audio_gen.train_audio_paths)\n",
    "    steps_per_epoch = num_train_examples//minibatch_size\n",
    "    # calculate validation_steps\n",
    "    num_valid_samples = len(audio_gen.valid_audio_paths) \n",
    "    validation_steps = num_valid_samples//minibatch_size\n",
    "    \n",
    "    # add CTC loss to the NN specified in input_to_softmax\n",
    "    model = add_ctc_loss(input_to_softmax)\n",
    "\n",
    "    # CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "\n",
    "    # make results/ directory, if necessary\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    # add checkpointer\n",
    "    checkpointer = ModelCheckpoint(filepath='/content/models/'+model_name+'.h5', verbose=0)\n",
    "\n",
    "    # train the model\n",
    "    hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, validation_data=audio_gen.next_valid(), validation_steps=validation_steps,\n",
    "        callbacks=[checkpointer], verbose=verbose, use_multiprocessing=True)\n",
    "\n",
    "    # save model loss\n",
    "    with open('/content/models/'+model_name+'.pickle', 'wb') as f:\n",
    "        pickle.dump(hist.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32b04e23-30ef-434e-978e-59f4113792ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/train_corpus.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m audio_gen \u001b[38;5;241m=\u001b[39m make_audio_gen(TRAIN_CORPUS, VALID_CORPUS, spectrogram\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mfcc_dim\u001b[38;5;241m=\u001b[39mMFCC_DIM,\n\u001b[1;32m     16\u001b[0m                            minibatch_size\u001b[38;5;241m=\u001b[39mMINI_BATCH_SIZE, sort_by_duration\u001b[38;5;241m=\u001b[39mSORT_BY_DURATION,\n\u001b[1;32m     17\u001b[0m                            max_duration\u001b[38;5;241m=\u001b[39mMAX_DURATION)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# add the training data to the generator\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43maudio_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m audio_gen\u001b[38;5;241m.\u001b[39mload_validation_data()\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mAudioGenerator.load_train_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_train_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    155\u001b[0m     desc_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_corpus\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_metadata_from_desc_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_train()\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_by_duration:\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mAudioGenerator.load_metadata_from_desc_file\u001b[0;34m(self, desc_file, partition)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\" Read metadata from a JSON-line file\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    (possibly takes long, depending on the filesize)\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03mParams:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    partition (str): One of 'train', 'validation' or 'test'\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m audio_paths, durations, texts \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdesc_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_line_file:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line_num, json_line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(json_line_file):\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train_corpus.json'"
     ]
    }
   ],
   "source": [
    "TRAIN_CORPUS = \"/content/train_corpus.json\"\n",
    "VALID_CORPUS = \"/content/valid_corpus.json\"\n",
    "\n",
    "MFCC_DIM = 13\n",
    "SPECTOGRAM = False\n",
    "EPOCHS = 5\n",
    "MODEL_NAME = \"RNN_model\"\n",
    "\n",
    "################ Reminder MINI_BATCH_SIZE=250 \n",
    "MINI_BATCH_SIZE = 250\n",
    "\n",
    "SORT_BY_DURATION=False\n",
    "MAX_DURATION = 10.0\n",
    "\n",
    "audio_gen = make_audio_gen(TRAIN_CORPUS, VALID_CORPUS, spectrogram=False, mfcc_dim=MFCC_DIM,\n",
    "                           minibatch_size=MINI_BATCH_SIZE, sort_by_duration=SORT_BY_DURATION,\n",
    "                           max_duration=MAX_DURATION)\n",
    "# add the training data to the generator\n",
    "audio_gen.load_train_data()\n",
    "audio_gen.load_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c544c7-2ec6-4b84-9994-b6a4f0d44bdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_1\u001b[49m(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m,\n\u001b[1;32m      2\u001b[0m                 units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      3\u001b[0m                 activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                 output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(char_map)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "model = model_1(input_dim=13,\n",
    "                units=5,\n",
    "                activation='relu',\n",
    "                output_dim=len(char_map)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36fcfa-c4a2-4c5a-8044-29c10906325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(audio_gen, input_to_softmax=model, model_name=MODEL_NAME, epochs=EPOCHS, minibatch_size=MINI_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e5ef4-bc67-47a5-8f4b-5d65dfc361ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_gen, index, partition, model, verbose=True):\n",
    "    \"\"\" Print a model's decoded predictions\n",
    "    Params:\n",
    "        data_gen: Data to run prediction on\n",
    "        index (int): Example to visualize\n",
    "        partition (str): Either 'train' or 'validation'\n",
    "        model (Model): The acoustic model\n",
    "    \"\"\"\n",
    "    audio_path,data_point,transcr,prediction = predict_raw(data_gen, index, partition, model)\n",
    "    output_length = [model.output_length(data_point.shape[0])]\n",
    "    pred_ints = (K.eval(K.ctc_decode(\n",
    "                prediction, output_length, greedy=False)[0][0])+1).flatten().tolist()\n",
    "    predicted = ''.join(int_sequence_to_text(pred_ints)).replace(\"<SPACE>\", \" \")\n",
    "    wer_val = wer(transcr, predicted)\n",
    "    if verbose:\n",
    "        display(Audio(audio_path, embed=True))\n",
    "        print('Truth: ' + transcr)\n",
    "        print('Predicted: ' + predicted)\n",
    "        print(\"wer: %d\" % wer_val)\n",
    "    return wer_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db478c87-7bc0-4e54-bb60-339f0e62e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_raw(data_gen, index, partition, model):\n",
    "    \"\"\" Get a model's decoded predictions\n",
    "    Params:\n",
    "        data_gen: Data to run prediction on\n",
    "        index (int): Example to visualize\n",
    "        partition (str): Either 'train' or 'validation'\n",
    "        model (Model): The acoustic model\n",
    "    \"\"\"\n",
    "\n",
    "    if partition == 'validation':\n",
    "        transcr = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    elif partition == 'train':\n",
    "        transcr = data_gen.train_texts[index]\n",
    "        audio_path = data_gen.train_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
    "        \n",
    "    prediction = model.predict(np.expand_dims(data_point, axis=0))\n",
    "    return (audio_path,data_point,transcr,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e78c6-9954-4f44-a96c-082be93dfad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer(model, model_name, data_gen, partition, length):\n",
    "    start = time.time()\n",
    "    def wer_single(i):\n",
    "        wer = predict(data_gen, i, partition, model, verbose=False)\n",
    "        if (i%100==0) and i>0:\n",
    "            print(\"processed %d in %d minutes\" % (i, ((time.time() - start)/60)))\n",
    "        return wer\n",
    "    wer = list(map(lambda i: wer_single(i), range(1, length)))\n",
    "    with open(\"metrics.txt\", 'w') as outfile:\n",
    "            outfile.write(\"Word Error Rate: %2.1f%%\\n\" % wer)\n",
    "    print(\"Total time: %f minutes\" % ((time.time() - start)/60))\n",
    "    filename = 'content/models/' + model_name + '_' + partition + '_wer.pickle'\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(wer, handle)\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c6159-3dbe-4424-ab7c-009712e8467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wer(model_name, partition):\n",
    "    filename = 'content/models/' + model_name + '_' + partition + '_wer.pickle'\n",
    "    return pickle.load(open(filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79688624-1c5c-4330-8f55-29e7a7f0ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw(name):\n",
    "    raw_audio = librosa.load(\"/content/ALFFA_PUBLIC/ASR/SWAHILI/data/test/wav5/data/SWH-05-20101124/\" + name)[0]\n",
    "    plot_raw_audio(raw_audio, size=(12,2))\n",
    "\n",
    "def plot_spect(name):\n",
    "    plot_spectrogram_feature(spectrogram_from_file(\"/content/ALFFA_PUBLIC/ASR/SWAHILI/data/test/wav5/data/SWH-05-20101124/\" + name))\n",
    "\n",
    "def plot_mfcc(name):\n",
    "    (rate, sig) = wav.read(\"/content/ALFFA_PUBLIC/ASR/SWAHILI/data/test/wav5/data/SWH-05-20101124/\" + name)\n",
    "    plot_mfcc_feature(mfcc(sig, rate, numcep=13))\n",
    "\n",
    "plot_mfcc(\"16k-emission_swahili_05h30_-_06h00_tu_20101124_part001g.wav\")\n",
    "plot_mfcc(\"16k-emission_swahili_05h30_-_06h00_tu_20101124_part002m.wav\")\n",
    "\n",
    "\n",
    "plot_spect(\"16k-emission_swahili_05h30_-_06h00_tu_20101124_part001g.wav\")\n",
    "plot_spect(\"16k-emission_swahili_05h30_-_06h00_tu_20101124_part001g.wav\")\n",
    "\n",
    "\n",
    "plot_raw(\"16k-emission_swahili_05h30_-_06h00_tu_20101124_part001g.wav\")\n",
    "plot_raw(\"16k-emission_swahili_05h30_-_06h00_tu_20101124_part001g.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa331077-ff3e-49b0-b25a-797250217aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_1(input_dim=13,\n",
    "                units=5,\n",
    "                activation='relu',\n",
    "                output_dim=len(char_map)+1)\n",
    "model.load_weights('/content/models/RNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ebac6-ac70-4aa5-8d6d-4258fd1ad29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map[0]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253a5f9-ba12-4737-9bde-c45687d85751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3688d5-0101-46cc-b0c0-ab9819eb63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(audio_gen,14, 'train', model)\n",
    "_,_,_,raw_pred = predict_raw(audio_gen,14, 'train', model)\n",
    "raw_pred_char = np.vstack([sorted(char_map.keys(), key=lambda k: char_map[k]) + ['BLANK'], raw_pred[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416f2c8-8859-4e32-ae8c-097341382528",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc624d-c765-44aa-8531-fda4ee0de0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f053a9-d4e5-4af0-9a9f-29f057fdf2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16934bc-0bb4-4fe5-befb-33264ffdbfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2a171-cac1-4dcc-822d-9e3259f0db7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9416b4-d231-4123-b16b-876d225a083b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
