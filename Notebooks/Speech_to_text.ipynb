{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e544c3b0-b8ef-4048-849a-0445ac6c380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c77f48-3703-4a3c-a7bd-bc0afa36439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# import os\n",
    "# from getpass import getpass\n",
    "\n",
    "# os.environ['MLFLOW_TRACKING_USERNAME'] = input('Enter your DAGsHub username: ')\n",
    "# os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your DAGsHub access token: ')\n",
    "# os.environ['MLFLOW_TRACKING_PROJECTNAME'] = input('Enter your DAGsHub project name: ')\n",
    "\n",
    "# mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] \n",
    "#                         + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
    "\n",
    "# with mlflow.start_run(run_name=\"MLflow on Colab\"):\n",
    "#   mlflow.log_metric(\"m1\", 2.0)\n",
    "#   mlflow.log_param(\"p1\", \"mlflow-colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87125299-d98b-42fc-af4a-0f6f9201d797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c561a08-018d-4169-8dda-a6a69435527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import math, random\n",
    "from torchaudio import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec82ea03-9a54-4926-a98d-8e8234a03526",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_to_train = \"/home/david/Elvis/MLOps_Strategist/ALFFA_PUBLIC/ASR/SWAHILI/data/train/wav\"\n",
    "Path_to_test = \"/home/david/Elvis/MLOps_Strategist/ALFFA_PUBLIC/ASR/SWAHILI/data/test/wav5\"\n",
    "Path_to_train_labels = \"/home/david/Elvis/MLOps_Strategist/ALFFA_PUBLIC/ASR/SWAHILI/data/train/text\"\n",
    "Path_to_test_labels=\"/home/david/Elvis/MLOps_Strategist/ALFFA_PUBLIC/ASR/SWAHILI/data/test/text\"\n",
    "Path_to_spectrograms = \"/spectrograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d253f6d2-7b31-4ccf-80d1-9eba4e72fb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/david/Elvis/MLOps_Strategist/ALFFA_PUBLIC/ASR/SWAHILI/data/train/text'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path_to_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a931f11-ddf4-489a-9bcb-41acfb8cc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all .wav from subdirectories\n",
    "def get_file_paths(dirname):\n",
    "    file_paths = []  \n",
    "    for root, directories, files in os.walk(dirname):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)  \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa206373-f3cf-4535-8389-349e0c077770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_paths(Path_to_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11da91ef-f75a-4fb5-87bf-bfe460d4f8c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/all_files/SWH-05-20110114_16k-emission_swahili_05h30_-_06h00_tu_20110114_part91.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m src \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirpath, filename)\n\u001b[1;32m     10\u001b[0m dest \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_path, filename)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy2\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/shutil.py:435\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    434\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 435\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/usr/lib/python3.8/shutil.py:264\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    262\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc, \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/all_files/SWH-05-20110114_16k-emission_swahili_05h30_-_06h00_tu_20110114_part91.wav'"
     ]
    }
   ],
   "source": [
    "#create a folder with all the train files\n",
    "import shutil\n",
    "in_path = Path_to_train\n",
    "out_path = \"/all_files\"\n",
    " \n",
    "for dirpath, dirnames, filenames in os.walk(in_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".wav\"):\n",
    "             src = os.path.join(dirpath, filename)\n",
    "             dest = os.path.join(out_path, filename)\n",
    "             shutil.copy2(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8548cd-4516-49a1-b4fb-943c7734510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the data\n",
    "# def process_file(file):\n",
    "#     r = sr.Recognizer()\n",
    "#     a = ''\n",
    "#     with sr.AudioFile(file) as source:\n",
    "#         audio = r.record(source)    \n",
    "#         try:\n",
    "#             a =  r.recognize_google(audio)        \n",
    "#         except sr.UnknownValueError:\n",
    "#             a = \"Google Speech Recognition could not understand audio\"\n",
    "#         except sr.RequestError as e:\n",
    "#             a = \"Could not request results from Google Speech Recognition service; {0}\".format(e)  \n",
    "#     return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b8c2771-6211-4dfd-958b-1b688b45b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     files = get_file_paths(Path_to_train)                 # get all file-paths of all files in dirname and subdirectories\n",
    "#     for file in files:                              # execute for each file\n",
    "#         (filepath, ext) = os.path.splitext(file)    # get the file extension\n",
    "#         file_name = os.path.basename(file)          # get the basename for writing to output file\n",
    "#         if ext == '.wav':                           # only interested if extension is '.wav'\n",
    "#             a = process_file(file)                  # result is returned to a\n",
    "#             with open(OUTPUTFILE, 'a') as f:        # write results to file\n",
    "#                 writer = csv.writer(f)\n",
    "#                 writer.writerow(['file_name','google'])\n",
    "#                 writer.writerow([file_name, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7af0504-7b96-45fd-b28f-7495b00215ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #put extracted .wav files into a dataframe\n",
    "# speech_df = pd.DataFrame (get_file_paths(Path_to_train),columns=['recording'])\n",
    "# speech_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "777f003d-0238-4cbf-9f9c-315ae7f9989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #extract content of text file(labels)\n",
    "# text_labels=pd.DataFrame(pd.read_csv(Path_to_train_labels, sep=',',header=None, names=['file_path'])['file_path'].str.split('\\t',1).tolist(),\n",
    "#                                  columns = ['file_path','text'])\n",
    "# text_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c4eb409-3239-4fa8-be10-175af629b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c224f2d5-0cfe-4a74-a187-38d55858ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dataframes based on the name of the file path use metadata\n",
    "\n",
    "#read text from every transcription audio\n",
    "def read_text( text_path):\n",
    "    text = []\n",
    "    with open(text_path) as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            text.append(line)\n",
    "            line = fp.readline()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c114397f-9829-4a89-a236-876193cc559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=[]\n",
    "transcriptions = []\n",
    "for t in read_text(Path_to_train_labels):\n",
    "    name=t.split(\"\\t\")[1]\n",
    "    # name=name.replace('(', '')\n",
    "    # name=name.replace(')', '')\n",
    "    name=name.replace('\\n','')\n",
    "    # name=name.replace(' ','')\n",
    "    text=t.split(\"\\t\")[0]\n",
    "    # text=text.replace(\"\\t\",\"\")\n",
    "    # name_to_text[name]=text\n",
    "    label.append(text)\n",
    "    transcriptions.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95967643-04ef-4e32-98cc-b2f690f22872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get audio path , every path must corespond to transcription , get the transprion in the doc and append to audio path \n",
    "audio_path=[0]*len(transcriptions)\n",
    "for d in get_file_paths(Path_to_spectrograms):\n",
    "    _d = d.strip(\".wav\")\n",
    "    sp = _d.split(\"/\")[9]\n",
    "    index = label.index(sp)\n",
    "    audio_path[index] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc891a6f-6240-4d32-8bfe-ff5db74d7b28",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error opening 0: File contains data in an unknown format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m duration_of_recordings\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m audio_path:\n\u001b[0;32m----> 4\u001b[0m     audio, fs \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     duration_of_recordings\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(audio)\u001b[38;5;241m/\u001b[39mfs))\n",
      "File \u001b[0;32m~/jupyter/lib/python3.8/site-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m~/jupyter/lib/python3.8/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m         y, sr_native \u001b[38;5;241m=\u001b[39m __audioread_load(path, offset, duration, dtype)\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m (exc)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Final cleanup for dtype and contiguity\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mono:\n",
      "File \u001b[0;32m~/jupyter/lib/python3.8/site-packages/librosa/core/audio.py:155\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    152\u001b[0m     context \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n\u001b[1;32m    158\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m sf_desc\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/jupyter/lib/python3.8/site-packages/soundfile.py:629\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    628\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/jupyter/lib/python3.8/site-packages/soundfile.py:1183\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file: \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m-> 1183\u001b[0m \u001b[43m_error_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_snd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msf_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_ptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError opening \u001b[39;49m\u001b[38;5;132;43;01m{0!r}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/jupyter/lib/python3.8/site-packages/soundfile.py:1357\u001b[0m, in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1356\u001b[0m     err_str \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error_number(err)\n\u001b[0;32m-> 1357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(prefix \u001b[38;5;241m+\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(err_str)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error opening 0: File contains data in an unknown format."
     ]
    }
   ],
   "source": [
    "#calculate duration \n",
    "duration_of_recordings=[]\n",
    "for d in audio_path:\n",
    "    audio, fs = librosa.load(d, sr=None)\n",
    "    duration_of_recordings.append(float(len(audio)/fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d93650-e00b-4be1-8497-93993f34488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_data=pd.DataFrame({'key': audio_path,'text': transcriptions})\n",
    "speech_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78935bd7-4e9b-49da-b9f5-6c5bd798d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66aa6d-4a3a-457c-a9e8-84e97ae9b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set energy threshold\n",
    "# audios that are below this threshold will be considered silent\n",
    "recognizer.energy_threshold = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3a4d10e-4ba4-43fe-b422-3ae1f7afa094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to load audio file\n",
    "#specifying sample rate will resize all the files i.e Audio will be automatically resampled to the given rate\n",
    "class Loader:\n",
    "  def __init__(self, sample_rate,duration,mono):\n",
    "    self.sample_rate=sample_rate\n",
    "    self.duration=duration\n",
    "    self.mono=mono\n",
    "    self.channel = 2\n",
    "    self.shift_limit = 0.4\n",
    "    self.n_mels=64 \n",
    "    self.n_fft=1024 \n",
    "    self.hop_len=None\n",
    "    self.max_mask_pct=0.1 \n",
    "    self.n_freq_masks=1 \n",
    "    self.n_time_masks=1\n",
    "    \n",
    "\n",
    "  # def load_signal(self, filepath):\n",
    "  #   signal=librosa.load(filepath,\n",
    "  #                       sr=None,\n",
    "  #                       duration=self.duration,\n",
    "  #                       mono=self.mono)[0]    #librosa returns 2D array (signal,sample_rate) pick the signal here\n",
    "  #   return signal\n",
    "\n",
    "  # def load_sample_rate(self,filepath):\n",
    "  #   self.sample_rate=librosa.load(filepath,\n",
    "  #                       sr =None,\n",
    "  #                       duration=self.duration,\n",
    "  #                       mono=self.mono)[1]    #librosa returns 2D array (signal,sample_rate) pick the sample rate here\n",
    "  #   return self.sample_rate\n",
    "\n",
    "  # def load(self,filepath):     #get both signal and sample rate in one\n",
    "  #   aud=librosa.load(filepath,\n",
    "  #                       sr=None,\n",
    "  #                       duration=self.duration,\n",
    "  #                       mono=self.mono)\n",
    "  #   return aud\n",
    "\n",
    "  def load(self,filepath):\n",
    "    sig, sr = torchaudio.load(filepath)\n",
    "    aud = sig, sr\n",
    "    return aud\n",
    "\n",
    "  def rechannel(self, aud):    #convert mono to stereo\n",
    "    # aud=self.aud\n",
    "    sig, sr = aud\n",
    "  \n",
    "\n",
    "    if (sig.shape[0] == self.channel):\n",
    "      # Nothing to do\n",
    "      return self.aud\n",
    "\n",
    "    if (self.channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig])\n",
    "\n",
    "    aud = resig, sr\n",
    "\n",
    "    return aud\n",
    "\n",
    "  def resample(self,aud):                    #standardize sample rate\n",
    "    sig, sr = aud\n",
    "    \n",
    "    if (sr == self.sample_rate):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, self.sample_rate)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, self.sample_rate)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "      aud = resig, self.sample_rate\n",
    "    return aud\n",
    "\n",
    "  # ----------------------------\n",
    "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
    "  # ----------------------------\n",
    "  def pad_trunc(self,aud):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * self.duration\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      aud = sig, sr\n",
    "    return aud\n",
    "\n",
    "# ----------------------------\n",
    "  # Shifts the signal to the left or right by some percent. Values at the end\n",
    "  # are 'wrapped around' to the start of the transformed signal.\n",
    "  # ----------------------------\n",
    "  def time_shift(self,aud):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * self.shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "  # ----------------------------\n",
    "  # Generate a MelSpectrogram\n",
    "  # ----------------------------\n",
    "  def spectro_gram(self, aud):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=self.n_fft, hop_length=self.hop_len, n_mels=self.n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "  # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "  # overfitting and to help the model generalise better. The masked sections are\n",
    "  # replaced with the mean value.\n",
    "  # ----------------------------\n",
    "  def spectro_augment(spec):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = self.max_mask_pct * n_mels\n",
    "    for _ in range(self.n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = self.max_mask_pct * n_steps\n",
    "    for _ in range(self.n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "857d6a6a-af44-4e01-abb8-d8eae0550cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #resizes audios to have same length\n",
    "# class Padder:\n",
    "#   def __init__(self,mode=\"constant\"):\n",
    "#     self.mode=mode\n",
    "#   def left_pad(self,array,num_missing_items):\n",
    "#     padded_array=np.pad(array,\n",
    "#                         (num_missing_items, 0),\n",
    "#                         mode=self.mode)\n",
    "#     return padded_array\n",
    "#   def right_pad(self,array,num_missing_items):\n",
    "#     padded_array=np.pad(array,\n",
    "#                         (0,num_missing_items),\n",
    "#                         mode=self.mode)\n",
    "#     return padded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4aaf5b-b5eb-4d01-9376-85d479ea23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save spectrograms in a folder\n",
    "class Saver:\n",
    "  def __init__(self,feature_save_dir):\n",
    "    self.feature_save_dir=feature_save_dir\n",
    "\n",
    "  def feature_save(self,spec,filepath):\n",
    "    save_path=self._generate_save_path(filepath)\n",
    "    np.save(save_path, spec)\n",
    "\n",
    "  def _generate_save_path(self, filepath):\n",
    "    file_name = os.path.split(filepath)[1]\n",
    "    save_path = os.path.join(self.feature_save_dir, file_name)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f3672a4-3f2a-48a9-9fa8-0ac2b10c5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call functions in Loader and Saver\n",
    "class PreprocessingPipeline:\n",
    "  '''Processes audio files in a directory by applying the following steps\n",
    "    1. Load the data, convert to mono and resample sampling rate\n",
    "    2. Pad the audio\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # self.padder=None\n",
    "    self.saver=None\n",
    "    self._loader=None\n",
    "    # self._num_expected_samples=None\n",
    "   \n",
    "\n",
    " \n",
    "  # def loader(self):\n",
    "  #   return self._loader\n",
    "\n",
    "\n",
    " \n",
    "  # def loader(self,loader):\n",
    "  #   self.loader=loader\n",
    "  #   self._num_expected_samples=int(loader.sample_rate*loader.duration)\n",
    "\n",
    "\n",
    "  def process(self,audio_files_directory):\n",
    "    for root, directories, files in os.walk(audio_files_directory):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            self._process_file(filepath)\n",
    "            print(f\"Processed file {filepath}\")\n",
    "    \n",
    "  def _process_file(self,filepath):\n",
    "    signal=self.loader.load(filepath)\n",
    "    signal = self.loader.rechannel(signal)\n",
    "    signal = self.loader.resample(signal)\n",
    "    # if self._is_padding_necessary(signal):\n",
    "    #   signal=self._apply_padding(signal)\n",
    "    signal = self.loader.pad_trunc(signal)\n",
    "    signal = self.loader.time_shift(signal)\n",
    "    spec = self.loader.spectro_gram(signal) \n",
    "    save_path=self.saver.feature_save(spec,filepath)\n",
    "\n",
    "    \n",
    "\n",
    "  # def _is_padding_necessary(self,signal):\n",
    "  #   self.num_expected_samples=int(loader.sample_rate*loader.duration)\n",
    "  #   if len(signal) < self.num_expected_samples:\n",
    "  #     return True\n",
    "  #   return False\n",
    "\n",
    "  # def _apply_padding(self,signal):\n",
    "  #   num_missing_samples=self.num_expected_samples - len(signal)\n",
    "  #   padded_signal = self.padder.right_pad(signal, num_missing_samples)\n",
    "  #   return padded_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f72a9d81-4db9-428e-ba35-bb7089e3c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION=4000\n",
    "SAMPLE_RATE=44100\n",
    "MONO=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b39c196b-e1d3-4612-9524-f6380992f474",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path_to_spectrograms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m loader\u001b[38;5;241m=\u001b[39mLoader(SAMPLE_RATE, DURATION, MONO)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# padder=Padder()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m saver \u001b[38;5;241m=\u001b[39m Saver(\u001b[43mPath_to_spectrograms\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path_to_spectrograms' is not defined"
     ]
    }
   ],
   "source": [
    "loader=Loader(SAMPLE_RATE, DURATION, MONO)\n",
    "# padder=Padder()\n",
    "saver = Saver(Path_to_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b7997-223e-42f2-a862-e4add1903603",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline=PreprocessingPipeline()\n",
    "preprocessing_pipeline.loader=loader\n",
    "# preprocessing_pipeline.padder=padder\n",
    "preprocessing_pipeline.saver=saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82704fbe-5b7a-4346-8dc7-7ee3c6ab1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline.process(Path_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa40da-f508-4507-9c96-b001e28b05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map_str = \"\"\"\n",
    " ' 0\n",
    " <SPACE> 1\n",
    " a 2\n",
    " b 3\n",
    " c 4\n",
    " d 5\n",
    " e 6\n",
    " f 7\n",
    " g 8\n",
    " h 9\n",
    " i 10\n",
    " j 11\n",
    " k 12\n",
    " l 13\n",
    " m 14\n",
    " n 15\n",
    " o 16\n",
    " p 17\n",
    " q 18\n",
    " r 19\n",
    " s 20\n",
    " t 21\n",
    " u 22\n",
    " v 23\n",
    " w 24\n",
    " x 25\n",
    " y 26\n",
    " z 27\n",
    " '' 28\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4faa3ca-8c87-4593-b829-6d0cf547dde4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_map_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             string\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_map[i])\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(string)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m text_transform \u001b[38;5;241m=\u001b[39m TextTransform(\u001b[43mchar_map_str\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char_map_str' is not defined"
     ]
    }
   ],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self,char_map_str):\n",
    "        self.char_map_str = char_map_str\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in self.char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[1] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('', ' ')\n",
    "\n",
    "text_transform = TextTransform(char_map_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f1ba2-661e-42fb-a838-7b7016861702",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform.text_to_int(speech_data['text'][2])\n",
    "# speech_data.head()\n",
    "# speech_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de80b53-de55-4de7-b820-4161954c8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "myds = speech_data\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939edb0-7ec2-4f0d-ad21-c55d0a216589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06197823-c4f8-490b-a0dc-cbb8f4c64bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b92053-b0eb-4ec0-809a-1e4506891c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
