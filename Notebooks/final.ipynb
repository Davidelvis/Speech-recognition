{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28f27e9-4eec-4f79-828f-8e1769a2b0fe",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "\n",
    "The project will use speech data and their transcriptions to train a speech to text model. The goal is to develop a model that can collect data through speech. Five deep learning models will be compared and the best model will be used in the prediction of text from speech input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c93ebe-7c29-4cd7-85b7-8511d2e2928b",
   "metadata": {},
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "\n",
    "The first step will be to preprocess the data and generate spectrograms from it. the resulting spectrogram from one of the audios is displayed below. The spectograms will be converted to Mel Frequency Ceptral coefficients(MFCC). The final models will allow the use of the dimensions of either a spectrogram or MFCC. MFCCs improve on the spectrogram by first taking into account the fact that humans passive speech on a logarithmic scale and the compresses the features and extracts the most common frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4db4856-e6a5-43c1-a8e2-b7b160c5e31d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vis_train_features\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_generator'"
     ]
    }
   ],
   "source": [
    "from data_generator import vis_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b996b4-cf35-4bd8-9c4e-f175058bad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract label and audio features for a single training example\n",
    "vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path = vis_train_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ef148-5110-491f-a433-4b6f49092e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from data_generator import vis_train_features, plot_raw_audio\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "\n",
    "# plot audio signal\n",
    "plot_raw_audio(vis_raw_audio)\n",
    "# print length of audio signal\n",
    "display(Markdown('**Shape of Audio Signal** : ' + str(vis_raw_audio.shape)))\n",
    "# print transcript corresponding to audio clip\n",
    "display(Markdown('**Transcript** : ' + str(vis_text)))\n",
    "# play the audio file\n",
    "Audio(vis_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177cb215-0df3-4da0-ba4a-9f717ce0cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import plot_spectrogram_feature\n",
    "\n",
    "# plot normalized spectrogram\n",
    "plot_spectrogram_feature(vis_spectrogram_feature)\n",
    "# print shape of spectrogram\n",
    "display(Markdown('**Shape of Spectrogram** : ' + str(vis_spectrogram_feature.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63951949-dad3-47be-b281-118865ce33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import plot_mfcc_feature\n",
    "\n",
    "# plot normalized MFCC\n",
    "plot_mfcc_feature(vis_mfcc_feature)\n",
    "# print shape of MFCC\n",
    "display(Markdown('**Shape of MFCC** : ' + str(vis_mfcc_feature.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311cc8e-ab19-45c6-89a3-03dba0fbe9ec",
   "metadata": {},
   "source": [
    "# TRAINING\n",
    "\n",
    "\n",
    "The dimensions extracted from the spectrograms and MFCCs are important for the next step of training with deep learning. We begin with a simple RNN model and add layers to it. The outputs for the models are vectors of the probabilities of a character being spoken. The CTC loss criterion is used to train the model. The CTC automatically maps input features to output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de743c-7c79-4285-a81e-07b42b7931aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # allocate 50% of GPU memory (if you like, feel free to change this)\n",
    "# from tensorflow.keras.backend.tensorflow_backend import set_session\n",
    "# import tensorflow as tf \n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "# # watch for any changes in the sample_models module, and reload it automatically\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import NN architectures for speech recognition\n",
    "from sample_models import *\n",
    "# import function for training acoustic model\n",
    "from train_utils import train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471589e-f5aa-44a7-9804-9234abb4f663",
   "metadata": {},
   "source": [
    "# SIMPLE RNN\n",
    "\n",
    "\n",
    "The recurrent neural network works by looking at past valuea as well as the information gained from them. The models are good for training data that is sequential like speech and text. For the simple RNN the output of the previuos timestep is fed into the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885f547-c7e6-4de2-8345-d14b7c27771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = simple_rnn_model(input_dim=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf20ead-ed7b-4bc3-ab30-6c8b89499e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_0, \n",
    "            pickle_path='model_0.pickle', \n",
    "            save_model_path='model_0.h5',\n",
    "            spectrogram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014d7a6-83ce-435d-9c0b-ccbe873bf36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = rnn_model(input_dim=13, \n",
    "                    units=200,\n",
    "                    activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf0703c-f971-4733-822c-281280217ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_1, \n",
    "            pickle_path='model_1.pickle', \n",
    "            save_model_path='model_1.h5',\n",
    "            spectrogram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf86c9-8a34-49cb-86a8-02abfe03fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_2, \n",
    "            pickle_path='model_2.pickle', \n",
    "            save_model_path='model_2.h5', \n",
    "            spectrogram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8d72e-7738-4db8-b7b0-f7928465931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = deep_rnn_model(input_dim=161,\n",
    "                         units=200,\n",
    "                         recur_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a04afb-87ca-4087-ae7b-97a5e80a3a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_3, \n",
    "            pickle_path='model_3.pickle', \n",
    "            save_model_path='model_3.h5', \n",
    "            spectrogram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0b855-9400-4418-a6c5-f65df2961a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = bidirectional_rnn_model(input_dim=161, \n",
    "                                  units=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab988ef7-96d2-4ca3-9fef-3e0659d5383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_4, \n",
    "            pickle_path='model_4.pickle', \n",
    "            save_model_path='model_4.h5', \n",
    "            spectrogram=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6dc80a-9389-453b-884d-ecea0f5f3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = input('Enter your DAGsHub username: ')\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your DAGsHub access token: ')\n",
    "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = input('Enter your DAGsHub project name: ')\n",
    "\n",
    "mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] \n",
    "                        + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
    "\n",
    "\n",
    "#   mlflow.log_metric(\"m1\", 2.0)\n",
    "#   mlflow.log_param(\"p1\", \"mlflow-colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089b3f7-e607-4185-9058-4fac0ae00a1b",
   "metadata": {},
   "source": [
    "The loss curves give an idea of the direction in which a model is learning and the loss curve for the five models above are plotted below. Model 4, with a bidirectional layer, appears to have a good learning rate. https://cs231n.github.io/neural-networks-3/#loss explains the variations in a loss function, for low learning rates the improvement is linear while higher learning rates lead to faster decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4a891-e3ce-46a6-9aef-59dcd27e24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style(style='white')\n",
    "\n",
    "with mlflow.start_run(run_name=\"speech-recognition\",nested=True):\n",
    "\n",
    "    # obtain the paths for the saved model history\n",
    "    all_pickles = sorted(glob(\"results/*.pickle\"))\n",
    "    # extract the name of each model\n",
    "    model_names = [item[8:-7] for item in all_pickles]\n",
    "    # extract the loss history for each model\n",
    "    valid_loss = [pickle.load( open( i, \"rb\" ) )['val_loss'] for i in all_pickles]\n",
    "    train_loss = [pickle.load( open( i, \"rb\" ) )['loss'] for i in all_pickles]\n",
    "    # save the number of epochs used to train each model\n",
    "    num_epochs = [len(valid_loss[i]) for i in range(len(valid_loss))]\n",
    "\n",
    "    fig = plt.figure(figsize=(16,5))\n",
    "\n",
    "    # plot the training loss vs. epoch for each model\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    for i in range(len(all_pickles)):\n",
    "        ax1.plot(np.linspace(1, num_epochs[i], num_epochs[i]), \n",
    "                train_loss[i], label=model_names[i])\n",
    "    # clean up the plot\n",
    "    ax1.legend()  \n",
    "    ax1.set_xlim([1, max(num_epochs)])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "\n",
    "    # plot the validation loss vs. epoch for each model\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    for i in range(len(all_pickles)):\n",
    "        ax2.plot(np.linspace(1, num_epochs[i], num_epochs[i]), \n",
    "                valid_loss[i], label=model_names[i])\n",
    "    # clean up the plot\n",
    "    ax2.legend()  \n",
    "    ax2.set_xlim([1, max(num_epochs)])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    #     plt.show()\n",
    "    plt.savefig(\"loss.png\")\n",
    "    mlflow.log_artifact(\"loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7a384-a057-41d3-b451-a7a9fc4605d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "display(IPython.display.IFrame(\"https://dagshub.com/\"+ os.environ['MLFLOW_TRACKING_USERNAME'] \n",
    "                        + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + \"/experiments/#/\",'100%',600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8f137-d3b0-4e9b-b461-b1051f41ea95",
   "metadata": {},
   "source": [
    "The model chosen for the speech to text model will be a bidirectional model with two layers because the future context is availabel for speech data and can be exploited to provide a better prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44502f-10d3-47b9-8c44-703690935306",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_end = final_model(input_dim=13, units=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0b453-430c-4296-9f42-b4baf2dbbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.tensorflow.autolog()\n",
    "mlflow.log_param(\"task\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42969d57-2864-4728-84b3-409720fa73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_end, \n",
    "            pickle_path='model_end.pickle', \n",
    "            save_model_path='model_end.h5', \n",
    "            spectrogram=False) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d1afb-29fd-4ff8-b955-5f190154f21e",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96133d87-4fbe-4cb8-a4ba-3f82f9596d03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioGenerator\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Markdown, display\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_generator'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from data_generator import AudioGenerator\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "from utils import calc_feat_dim, spectrogram_from_file, text_to_int_sequence,int_sequence_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e95ce-6f1b-4a02-8ed8-31d673c9d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://martin-thoma.com/word-error-rate-calculation/\n",
    "def wer(r, h):\n",
    "    \"\"\"\n",
    "    Calculation of WER with Levenshtein distance.\n",
    "\n",
    "    Works only for iterables up to 254 elements (uint8).\n",
    "    O(nm) time ans space complexity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : list\n",
    "    h : list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
    "    1\n",
    "    >>> wer(\"who is there\".split(), \"\".split())\n",
    "    3\n",
    "    >>> wer(\"\".split(), \"who is there\".split())\n",
    "    3\n",
    "    \"\"\"\n",
    "    # initialisation\n",
    "    import numpy\n",
    "    d = numpy.zeros((len(r)+1)*(len(h)+1), dtype=numpy.uint8)\n",
    "    d = d.reshape((len(r)+1, len(h)+1))\n",
    "    for i in range(len(r)+1):\n",
    "        for j in range(len(h)+1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitution = d[i-1][j-1] + 1\n",
    "                insertion    = d[i][j-1] + 1\n",
    "                deletion     = d[i-1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    return d[len(r)][len(h)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcac43-fb43-4ce2-a4ae-4e029c346439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import AudioGenerator\n",
    "RNG_SEED = 123\n",
    "model = model_end\n",
    "model.load_weights('results/model_end.h5')\n",
    "\n",
    "def make_audio_gen(train_json,\n",
    "                   valid_json,\n",
    "                   minibatch_size=20,\n",
    "                   spectrogram=True,\n",
    "                   mfcc_dim=13,\n",
    "                   sort_by_duration=False,\n",
    "                   max_duration=10.0):\n",
    "    return AudioGenerator(minibatch_size=minibatch_size, \n",
    "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
    "        sort_by_duration=sort_by_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd134c9-3e34-4360-8e6c-7e811de80dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CORPUS = \"train_corpus.json\"\n",
    "VALID_CORPUS = \"valid_corpus.json\"\n",
    "\n",
    "MFCC_DIM = 13\n",
    "SPECTOGRAM = False\n",
    "EPOCHS = 5\n",
    "MODEL_NAME = \"RNN_model\"\n",
    "\n",
    "################ Reminder MINI_BATCH_SIZE=250 \n",
    "MINI_BATCH_SIZE = 20\n",
    "\n",
    "SORT_BY_DURATION=False\n",
    "MAX_DURATION = 10\n",
    "\n",
    "audio_gen = make_audio_gen(TRAIN_CORPUS, VALID_CORPUS, spectrogram=False, mfcc_dim=MFCC_DIM,\n",
    "                           minibatch_size=MINI_BATCH_SIZE, sort_by_duration=SORT_BY_DURATION,\n",
    "                           max_duration=MAX_DURATION)\n",
    "# add the training data to the generator\n",
    "audio_gen.load_train_data()\n",
    "audio_gen.load_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9131f-260f-4ca4-8d42-d978d25c6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_raw(data_gen = audio_gen, index = 14, partition = 'train', model = model):\n",
    "    \"\"\" Get a model's decoded predictions\n",
    "    Params:\n",
    "        data_gen: Data to run prediction on\n",
    "        index (int): Example to visualize\n",
    "        partition (str): Either 'train' or 'validation'\n",
    "        model (Model): The acoustic model\n",
    "    \"\"\"\n",
    "\n",
    "    if partition == 'validation':\n",
    "        transcr = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    elif partition == 'train':\n",
    "        transcr = data_gen.train_texts[index]\n",
    "        audio_path = data_gen.train_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
    "        \n",
    "    prediction = model.predict(np.expand_dims(data_point, axis=0))\n",
    "    return (audio_path,data_point,transcr,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429afec9-5872-4d16-9b8c-6ee18a7b24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_gen=audio_gen, index=14, partition = 'train', model=model, verbose=True):\n",
    "    \"\"\" Print a model's decoded predictions\n",
    "    Params:\n",
    "        data_gen: Data to run prediction on\n",
    "        index (int): Example to visualize\n",
    "        partition (str): Either 'train' or 'validation'\n",
    "        model (Model): The acoustic model\n",
    "    \"\"\"\n",
    "    audio_path,data_point,transcr,prediction = predict_raw(data_gen, index, partition, model)\n",
    "    output_length = [model.output_length(data_point.shape[0])]\n",
    "    pred_ints = (K.eval(K.ctc_decode(\n",
    "                prediction, output_length, greedy=False)[0][0])+1).flatten().tolist()\n",
    "    predicted = ''.join(int_sequence_to_text(pred_ints)).replace(\"<SPACE>\", \" \")\n",
    "    wer_val = wer(transcr, predicted)\n",
    "    if verbose:\n",
    "        display(Audio(audio_path, embed=True))\n",
    "        print('Truth: ' + transcr)\n",
    "        print('Predicted: ' + predicted)\n",
    "        print(\"wer: %d\" % wer_val)\n",
    "        # Write results to a file\n",
    "    with open(\"metrics.txt\", 'w') as outfile:\n",
    "            outfile.write(\"Text input: %s\\n\" % transcr)\n",
    "            outfile.write(\"Predicted Text: %s\\n\" % predicted)\n",
    "            outfile.write(\"Word Error Rate: %2.1f%%\\n\" % wer_val)\n",
    "    with open('test.wav', 'wb') as f:\n",
    "            f.write(Audio(audio_path).data)\n",
    "    return wer_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381c2af-7ebb-46b6-912d-af553185f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2bfaf-3e41-41ec-a787-1ddb40586bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfdccb-1533-4bfb-af21-c7a63fce500a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b6d24-e2d6-4b33-b8bd-63fff3667b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bd1a9-6e97-4216-83ea-fed4bf8209c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
